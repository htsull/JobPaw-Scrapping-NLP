{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the execel file generated from the getLinksScript.py script\n",
    "links = pd.read_excel('JobPawLinks.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                  'Chrome/100.0.4896.88 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some cleanup words to remove from the qualifications \n",
    "keys_to_delete = {\n",
    "    ord('\\r'): None, \n",
    "    ord('\\t'): None, \n",
    "    ord('¥'): None, \n",
    "    ord('…'): None, \n",
    "    ord('•'): None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = links.link\n",
    "# instantiate the list of jobs details\n",
    "allJobDetailsList = []\n",
    "\n",
    "def getJobDetails(url):\n",
    "    # tryin to get the job details\n",
    "    try:\n",
    "        # request the page from the url\n",
    "        r = requests.get(url, headers=headers)\n",
    "        # creating the soup\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        # caturin the table\n",
    "        jobTable = soup.findAll('table', {'class': 'table-bordered'})\n",
    "        # capturing the job qualifications\n",
    "        # the qualifications are in the jobTable after the 'strong' tag\n",
    "        # so we need to find the 'strong' tag and then the next sibling because all the pages \n",
    "        # dont have the same structure\n",
    "        qualif = soup.find('strong', string ='Qualifications réquises').find_next('p').text\n",
    "\n",
    "        # creting the list of jobs details\n",
    "        # the list of jobs details is a dictionary made with all the cells of the jobTable\n",
    "        # but to do so we need to locate the specific text of the cells, find the next sibling\n",
    "        # and then remove the unwanted characters\n",
    "        allJobDetails = dict(\n",
    "            title = jobTable[1].find('td', string='Titre du poste').find_next('td').text.strip(),\n",
    "            company = jobTable[1].find('td', string='Compagnie').find_next('td').text.strip(),\n",
    "            domain = jobTable[1].find('td', string='Domaine').find_next('td').text.strip(),\n",
    "            speciality = jobTable[1].find('td', string='Spécialité').find_next('td').text.strip(),\n",
    "            publicationDate = jobTable[1].find('td', string='Date publication').find_next('td').text.strip(),\n",
    "            limitDate = jobTable[1].find('td', string='Date limite').find_next('td').text.strip(),\n",
    "            country = jobTable[1].find('td', string='Pays').find_next('td').text.strip(),\n",
    "            town = jobTable[1].find('td', string='Ville').find_next('td').text.strip(),\n",
    "            zone = jobTable[1].find('td', string='Zone').find_next('td').text.strip(),\n",
    "            duration = jobTable[1].find('td', string='Durée').find_next('td').text.strip(),\n",
    "            qualification = qualif.translate(keys_to_delete).strip().split('\\n')\n",
    "        )\n",
    "        # append the job details to the list of jobs details\n",
    "        allJobDetailsList.append(allJobDetails)\n",
    "    # if the request fails, print the url and the error\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(url)\n",
    "            \n",
    "    return allJobDetailsList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a progress bar to make things looks nice\n",
    "# credit to https://stackoverflow.com/a/61295200/9915482\n",
    "def print_progressbar(total, current, barsize=60):\n",
    "    progress = int(current * barsize / total)\n",
    "    completed = str(int(current * 100 / total)) + '%'\n",
    "    print('[', chr(9608) * progress, ' ', completed, '.' * (barsize - progress), '] ',\n",
    "          str(current) + '/' + str(total), sep='', end='\\r', flush=True)\n",
    "\n",
    "print_frequency = max(min(len(urls) // 50, 100), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='www.jobpaw.com', port=443): Max retries exceeded with url: /pont/professionnels.php?idj=7678 (Caused by SSLError(SSLError(\"bad handshake: SysCallError(10054, 'WSAECONNRESET')\")))\n",
      "https://www.jobpaw.com/pont/professionnels.php?idj=7678\n",
      "[████████████████████████████████████████████████████████████ 100%] 12066/12066\r"
     ]
    }
   ],
   "source": [
    "for index, url in enumerate(urls):\n",
    "    if index % 500 == 0 and index != 0:\n",
    "        getJobDetails(url)\n",
    "        # sleep every 500 requests for a while to avoid getting banned\n",
    "        time.sleep(60)\n",
    "    else :\n",
    "        getJobDetails(url)\n",
    "    print_progressbar(len(urls), index + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the list of jobs details to an excel file\n",
    "pd.DataFrame(allJobDetailsList).to_excel('jobDetails.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export the list of jobs details to a json file\n",
    "# with open('jobDetails.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(allJobDetailsList, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.get(links.link[9], headers=headers)\n",
    "# soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# jobTable = soup.findAll('table', {'class': 'table-bordered'})\n",
    "# country = jobTable[1].find('td', string='Date publication').find_next('td').text.strip()\n",
    "# country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url0 = links.link[0]\n",
    "# url0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.get(url0, headers=headers)\n",
    "# soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# jobTable = soup.find('table', {'class' : 'table table-bordered'})\n",
    "# jobTable.find_all('strong')[2].find_next().text.strip().translate(keys_to_delete).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url1 = links.link[1]\n",
    "# url1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url2 = links.link[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\bsully\\OneDrive - Université Paris 1 Panthéon-Sorbonne\\Documents\\GitHub\\JobPaw-Scrapping-NLP\\getJobDetails.ipynb Cell 16'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/bsully/OneDrive%20-%20Universit%C3%A9%20Paris%201%20Panth%C3%A9on-Sorbonne/Documents/GitHub/JobPaw-Scrapping-NLP/getJobDetails.ipynb#ch0000008?line=0'>1</a>\u001b[0m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mstrong\u001b[39m\u001b[39m'\u001b[39m, string \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mQualifications réquises\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mfind_next(\u001b[39m'\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mtranslate(keys_to_delete)\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "soup.find('strong', string ='Qualifications réquises').find_next('p').text.translate(keys_to_delete).strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find('table', {'class' : 'table table-bordered'}).find_all('strong')[2].find_text()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a94f09328fd3e2fe90580e1b0af4127392053aec22d41566fb9ed67e92f0e9f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
